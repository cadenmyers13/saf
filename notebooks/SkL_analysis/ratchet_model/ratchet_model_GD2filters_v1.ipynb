{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Note: This will run if you have the the movies saved as \"start_numor\".npz files. Check other notebook titled SANS_to_npz.ipynb for code to assist in doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.signal import savgol_filter\n",
    "#from bg_mpl_stylesheets.styles import all_styles\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "#plt.style.use(all_styles[\"bg_style\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask for images\n",
    "\n",
    "def mask_and_blur_images(movie, sigma=.65):\n",
    "    '''masks signal inside radius of 14 and outside radius of 30 and adds gaussian blur for all intensity data'''\n",
    "    #print(f\"movie type: {type(movie)}, movie value: {movie}\")\n",
    "    for i in range(0, movie.shape[0]):\n",
    "        x,y = np.meshgrid(np.arange(128), np.arange(128))\n",
    "        radius = np.sqrt((x-64)**2 + (y-62)**2)\n",
    "        mask1 = radius <= 14\n",
    "        mask2 = radius >= 30\n",
    "        masked_data = movie[i].copy()\n",
    "        masked_data[mask1] = 0\n",
    "        masked_data2 = masked_data.copy()\n",
    "        masked_data2[mask2] = 0\n",
    "        # masked_data_norm = (masked_data - np.min(masked_data) / (np.max(masked_data) - np.min(masked_data)))\n",
    "        blurred_data = gaussian_filter(masked_data2, sigma=sigma)\n",
    "        movie[i] = blurred_data\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import images from .npz files\n",
    "ms = torch.arange(12)\n",
    "angles_rad = torch.arange(0, 6) * 2 * torch.pi / 6.\n",
    "\n",
    "# Extract data_theta, doesn't matter what images is extracted since we're just getting theta\n",
    "data = np.load(r\"C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Angle reference file (random file from Caden)\\image_111010.npz\")['data']\n",
    "data_theta = torch.atan2(torch.tensor(data[1]), torch.tensor(data[0]))\n",
    "\n",
    "# Extract data file paths\n",
    "file_path = r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\HDF to npz files\\\\'\n",
    "movies = ['Field_29mT.npz', 'Field_31mT.npz', 'Field_32mT.npz', 'Field_33mT.npz', 'Field_37mT.npz']\n",
    "\n",
    "# define the movie you want to run GD and GS on as gif\n",
    "gif = movies[0]\n",
    "print(gif)\n",
    "movie = np.load(file_path + gif)\n",
    "\n",
    "#shape (<movie length>,128,128)\n",
    "intensity_data = movie['data']\n",
    "print(intensity_data.shape[0])\n",
    "intensity_data = torch.tensor(mask_and_blur_images(intensity_data))\n",
    "# Apply a log transformation to the intensity data\n",
    "# Add a small constant (e.g., 1e-6) to avoid taking log(0)\n",
    "#intensity_data = torch.log(intensity_data + 1e-6)\n",
    "print(intensity_data.numpy().shape)\n",
    "initial_offset = torch.tensor(0., requires_grad=True)\n",
    "# plt.imshow(intensity_data[0])\n",
    "# plt.show()\n",
    "# plt.imshow(mask_and_blur_images(intensity_data.numpy())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "\n",
    "def normalize_min_max(array):\n",
    "    array_min = np.min(array)\n",
    "    array_max = np.max(array)\n",
    "    return (array - array_min) / (array_max - array_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the filter\n",
    "\n",
    "def project_theta(theta, m_values):\n",
    "    projections = []\n",
    "    for m in m_values:\n",
    "        sin_m_theta = torch.sin(m * theta )\n",
    "        cos_m_theta = torch.cos(m * theta )\n",
    "        projected_vectors = torch.stack((cos_m_theta, sin_m_theta), axis=-1)\n",
    "        projections.append(projected_vectors)\n",
    "    return torch.stack(projections, axis=0)\n",
    "\n",
    "def evaluate_functions_on_theta(theta, coefficients_list, m_values):\n",
    "    evaluated_function = torch.zeros(theta.shape, dtype=torch.float32)\n",
    "    for (a_cos, a_sin), m in zip(coefficients_list, m_values):\n",
    "        evaluated_function += a_sin * torch.sin(m * theta) + a_cos * torch.cos(m * theta)\n",
    "    return evaluated_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent function\n",
    "\n",
    "def optimize_offset(intensity, offset):\n",
    "    max_iter = 101\n",
    "    opt = torch.optim.Adam([offset], lr=1e-2)\n",
    "    for i in range(max_iter):\n",
    "        projection = project_theta(angles_rad + offset, ms).sum(1)\n",
    "        evaluate_image_theta = evaluate_functions_on_theta(data_theta, projection, ms)\n",
    "        loss = -(intensity * evaluate_image_theta).sum()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #if i % 100 == 0:\n",
    "        #    print(loss.item(), offset.item())\n",
    "    return offset, evaluate_image_theta, intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif gradient descent (for comparison with grid search function)\n",
    "gradient_descent_offset = []\n",
    "initial_offset = torch.tensor(0., requires_grad=True)\n",
    "print('data for movie', gif)\n",
    "for i in range(intensity_data.shape[0]):\n",
    "    offset, image, int = optimize_offset(intensity_data[i], initial_offset)\n",
    "    gradient_descent_offset.append(np.rad2deg(offset.item()))\n",
    "    signal = normalize_min_max(image.detach().numpy()) + normalize_min_max(int.detach().numpy())\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow(signal, cmap='jet', vmin=0, vmax=2)\n",
    "    axs[1].imshow(int, cmap='jet')\n",
    "    plt.show()\n",
    "    print(f'{i*10}s:', np.rad2deg(offset.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_offset(movie, offset_step=.2, angle_below_offset=8, angle_above_offset=3):\n",
    "    # offset_step_rad = np.deg2rad(offset_step)\n",
    "    optimal_offset = []\n",
    "\n",
    "    for index, image in enumerate(movie):\n",
    "        loss_offset = []\n",
    "        if index == 0:\n",
    "            offset_angles_deg = np.arange(-45, 45, offset_step)\n",
    "        else:\n",
    "            offset_angles_deg = np.arange(best_offset-angle_below_offset, best_offset+angle_above_offset, offset_step)\n",
    "\n",
    "        for offset_angle in offset_angles_deg:\n",
    "            offset = torch.tensor([offset_angle], dtype=torch.float32)\n",
    "            projection = project_theta(angles_rad + np.deg2rad(offset), ms).sum(1)\n",
    "            evaluate_image_theta = evaluate_functions_on_theta(data_theta, projection, ms)\n",
    "            image = image.numpy()\n",
    "            image = normalize_min_max(image)\n",
    "            image = torch.tensor(image)\n",
    "            loss = -(image * evaluate_image_theta).sum()\n",
    "            loss_offset.append((offset_angle, loss))\n",
    "        min_loss = min(loss_offset, key=lambda x: x[1])\n",
    "        best_offset = min_loss[0]\n",
    "\n",
    "        # plotting\n",
    "        norm_int = normalize_min_max(image.numpy())\n",
    "        norm_filter = normalize_min_max(evaluate_image_theta.numpy())\n",
    "        signal = norm_int + norm_filter\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axs[0].imshow(signal, cmap='jet', vmin=0, vmax=2)\n",
    "        axs[1].imshow(norm_int, cmap='jet')\n",
    "        plt.show()\n",
    "        optimal_offset.append(best_offset)\n",
    "        print(f'{index*10}s: ', best_offset)\n",
    "\n",
    "    return optimal_offset\n",
    "\n",
    "ratchet_offset = grid_search_offset(intensity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data created from above GD and GS\n",
    "\n",
    "time = np.array(range(len(ratchet_offset))) * 10\n",
    "#print(gradient_descent_offset[40])\n",
    "print(ratchet_offset[0])\n",
    "#plt.plot(time, ratchet_offset, label='ratchet model', color='blue', alpha=.7)\n",
    "plt.plot(time, gradient_descent_offset, label='gradient descent', color='red', alpha=.7)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('offset angle (deg)')\n",
    "#plt.title(gif.removesuffix('.npz'))\n",
    "plt.grid(True)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save Data to .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data and figure\n",
    "plt.plot(time, ratchet_offset, label='ratchet model', color='blue', alpha=.7)\n",
    "#plt.plot(time, gradient_descent_offset, label='gradient descent', color='red', alpha=.7)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('offset angle (deg)')\n",
    "plt.title(gif)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "# plt.savefig(f'/Users/cadenmyers/billingelab/dev/skyrmion_lattices/comparing_models/plots/{gif.removesuffix('.npz')}_plot.png')\n",
    "\n",
    "# Save the gradient descent data\n",
    "#gradient_descent_offset=np.array(gradient_descent_offset)\n",
    "# np.savez('/Users/cadenmyers/billingelab/dev/skyrmion_lattices/comparing_models/gradient_offset/gradient_descent_'+gif, offset=gradient_descent_offset, time=time)\n",
    "\n",
    "# Save the ratchet model data\n",
    "ratchet_offset=np.array(ratchet_offset)\n",
    "# Define your file path\n",
    "file_path = r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\\\'\n",
    "\n",
    "# Concatenate the file path with the filename stored in gif\n",
    "full_path = file_path + gif\n",
    "\n",
    "# Save the npz file using the full path\n",
    "np.savez(full_path, gif, offset=ratchet_offset, time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Plot data from saved npz files\n",
    "### everything below this is data analysis which includes plotting offset angle vs. time and calculating gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trim cooldown image data (eliminate irrelevant signal)\n",
    "# cooldown_ratchet_model_data = np.load('/Users/cadenmyers/billingelab/dev/skyrmion_lattices/comparing_models/ratchet_offset/ratchet_model_121405.npz')\n",
    "# rm_time = cooldown_ratchet_model_data['time']\n",
    "# rm_offset = cooldown_ratchet_model_data['offset']\n",
    "\n",
    "# # Define the threshold\n",
    "# threshold = 2000\n",
    "\n",
    "# # Filter the data\n",
    "# mask = rm_time >= threshold\n",
    "# filtered_time = rm_time[mask]-2000\n",
    "# filtered_offset = rm_offset[mask]\n",
    "# filtered_offset = filtered_offset-filtered_offset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "# np.savez('/Users/cadenmyers/billingelab/dev/skyrmion_lattices/comparing_models/ratchet_offset/ratchet_model_121405.npz',\n",
    "#          time=filtered_time, offset=filtered_offset)\n",
    "# # print(filtered_time)\n",
    "# # print(filtered_offset)\n",
    "\n",
    "# cooldown_ratchet_model_data = np.load('/Users/cadenmyers/billingelab/dev/skyrmion_lattices/comparing_models/ratchet_offset/ratchet_model_121405.npz')\n",
    "# rm_time = cooldown_ratchet_model_data['time']\n",
    "# rm_offset = cooldown_ratchet_model_data['offset']\n",
    "\n",
    "# print(rm_time.shape)\n",
    "# print(rm_offset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data from .npz files\n",
    "\n",
    "#for gif in movies:\n",
    "#gradient_descent_data = np.load(rf'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\npz files\\testing\\{gif}')\n",
    "#gd_time = gradient_descent_data['time']\n",
    "#gd_offset = gradient_descent_data['offset']\n",
    "#plt.plot(gd_time, gd_offset, label=gif.removesuffix('.npz'), alpha=.7)\n",
    "\n",
    "#plt.xlabel('time (s)')\n",
    "#plt.xlim(0,600)\n",
    "#plt.ylabel('offset angle (deg)')\n",
    "#plt.title('gradient descent')\n",
    "#plt.grid(True)\n",
    "#plt.legend(loc='upper right', bbox_to_anchor=(1.35,1))\n",
    "\n",
    "#plt.savefig('/Users/cadenmyers/billingelab/dev/skyrmion_lattices/comparing_models/plots/gradient_descent_offset_plot.png')\n",
    "#plt.show()\n",
    "\n",
    "for gif in movies:\n",
    "    ratchet_model_data = np.load(rf'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\{gif}')\n",
    "    rm_time = ratchet_model_data['time']\n",
    "    rm_offset = ratchet_model_data['offset']\n",
    "    m, b = np.polyfit(rm_time, rm_offset, 1)\n",
    "    best_fit = m * rm_time + b\n",
    "    SS_tot = np.sum((rm_offset - np.mean(rm_offset))**2)\n",
    "    SS_res = np.sum((rm_offset - best_fit)**2)\n",
    "    R_squared = 1 - (SS_res / SS_tot)\n",
    "    range = np.abs(np.min(rm_offset)-np.max(rm_offset))\n",
    "    # print(f'{gif} R^2=', R_squared)\n",
    "    # print(f'{gif} range=', range)\n",
    "    if R_squared >= .95 or range<=8:\n",
    "        #plt.plot(rm_time, best_fit, label=gif.removesuffix('.npz'), alpha=.7)\n",
    "        plt.plot(rm_time, best_fit, label=gif, alpha=.7)\n",
    "    else:\n",
    "        #plt.plot(rm_time, rm_offset, label=gif.removesuffix('.npz'), alpha=.7)\n",
    "        plt.plot(rm_time, rm_offset, label=gif, alpha=.7)\n",
    "\n",
    "plt.xlabel('time (s)')\n",
    "plt.xlim(0,380)\n",
    "plt.ylabel('offset angle (deg)')\n",
    "plt.title('Ratchet Model')\n",
    "plt.grid(True)\n",
    "#plt.legend(loc='upper right', bbox_to_anchor=(1.35,1))\n",
    "plt.legend(loc='center right')\n",
    "plt.savefig(r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Exported Figures\\FieldSweepPositions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angular Velocity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_smoothed_derivative(time, offset, window_length=11, polyorder=2):\n",
    "    '''compute velocity of data after savgol_filter is applied'''\n",
    "    smoothed_angle = savgol_filter(offset, window_length=window_length, polyorder=polyorder)\n",
    "    time = np.array(time)\n",
    "    smoothed_derivative = (np.gradient(smoothed_angle, time))\n",
    "    return smoothed_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate angular velo and plot\n",
    "for gif in movies:\n",
    "    ratchet_model_data = np.load(rf'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\{gif}')\n",
    "    rm_time = ratchet_model_data['time']\n",
    "    rm_offset = ratchet_model_data['offset']\n",
    "    m, b = np.polyfit(rm_time, rm_offset, 1)\n",
    "    best_fit = m * rm_time + b\n",
    "    SS_tot = np.sum((rm_offset - np.mean(rm_offset))**2)\n",
    "    SS_res = np.sum((rm_offset - best_fit)**2)\n",
    "    R_squared = 1 - (SS_res / SS_tot)\n",
    "    range = np.abs(np.min(rm_offset)-np.max(rm_offset))\n",
    "    #print(f'{gif.removesuffix('.npz')} R^2=', R_squared)\n",
    "    print(f'{gif} R^2=', R_squared)\n",
    "    # print(f'{gif} range=', range)\n",
    "    if R_squared >= .98 or range<=8:\n",
    "        velo = compute_smoothed_derivative(rm_time, best_fit)\n",
    "        #print(f'{gif.removesuffix('.npz')} avg angular velo=', m, 'deg/s')\n",
    "        print(f'{gif} avg angular velo=', m, 'deg/s')\n",
    "    else:\n",
    "        velo = compute_smoothed_derivative(rm_time, rm_offset)\n",
    "\n",
    "    #plt.plot(rm_time, velo, label=gif.removesuffix('.npz'), alpha=.7)\n",
    "    plt.plot(rm_time, velo, label=gif, alpha=.7)\n",
    "\n",
    "plt.xlabel('time (s)')\n",
    "plt.xlim(0,380)\n",
    "plt.ylabel('Angular Velocity (deg/s)')\n",
    "plt.title('Ratchet Model')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.35,1))\n",
    "plt.savefig(r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Exported Figures\\FieldSweepVelocities.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (2 filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create filter\n",
    "def project_theta(theta, m_values):\n",
    "    projections = []\n",
    "    for m in m_values:\n",
    "        sin_m_theta = torch.sin(m * theta)\n",
    "        cos_m_theta = torch.cos(m * theta)\n",
    "        projected_vectors = torch.stack((cos_m_theta, sin_m_theta), axis=-1)\n",
    "        projections.append(projected_vectors)\n",
    "    return torch.stack(projections, axis=0)\n",
    "\n",
    "def evaluate_functions_on_theta(theta, coefficients_list, m_values):\n",
    "    evaluated_function = torch.zeros(theta.shape, dtype=torch.float32)    \n",
    "    for (a_cos, a_sin), m in zip(coefficients_list, m_values):\n",
    "        evaluated_function += a_sin * torch.sin(m * theta) + a_cos * torch.cos(m * theta)\n",
    "    return evaluated_function\n",
    "\n",
    "########## Create masks\n",
    "def normalize_image(image):\n",
    "    return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "def apply_threshold_to_sin(evaluate_image, threshold=-1):\n",
    "    \"\"\"Thin out the laser by creating a mask from `evaluate_image`, setting values below `threshold` to 0.\n",
    "    When `threshold = -1` this function doesn't change anything, when `threshold = 1` the image is completely white, \n",
    "    so typically threshold is between 0 and 1.\"\"\"\n",
    "    mask = torch.ones_like(evaluate_image, dtype=torch.float32)\n",
    "    evaluate_image = normalize_image(evaluate_image)\n",
    "    mask[evaluate_image < threshold] = 0\n",
    "    masked_image = evaluate_image * mask\n",
    "    return masked_image\n",
    "\n",
    "def create_mask_from_intensity(intensity, evaluate_image):\n",
    "    \"\"\"Mask regions in `evaluate_image` where the values are positive, setting these regions in `intensity` to 0.\n",
    "    This allows for a masked intensity image to be used in multiple filterings.\"\"\"\n",
    "    mask = torch.ones_like(evaluate_image, dtype=torch.float32)\n",
    "    mask[evaluate_image > 0] = 0\n",
    "    masked_intensity = intensity * mask\n",
    "    return masked_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent with 2 filters\n",
    "# threshold = -1 means no change to the laser\n",
    "# fix_snapback = True when you want to set a range to avoid snapback\n",
    "# initial tells if this is the first frame\n",
    "\n",
    "def optimize_offset_2filters(intensity, offset1, threshold=-1,\n",
    "                             initial=True, fix_snapback=False, angle_below_offset=51, angle_above_offset=9):\n",
    "\n",
    "    ########## start of the first filter\n",
    "    # Define parameters\n",
    "    max_iter_offset1 = 101\n",
    "    lr1 = 1e-2\n",
    "    opt1 = torch.optim.Adam([offset1], lr=lr1)\n",
    "\n",
    "    # Set offset1 range based on whether it's the first frame\n",
    "    offset1_range = (\n",
    "        (-30 / 180 * np.pi, 30 / 180 * np.pi) if initial \n",
    "        else (offset1 - angle_below_offset / 180 * np.pi, offset1 + angle_above_offset / 180 * np.pi)\n",
    "    )\n",
    "    \n",
    "    for i in range(max_iter_offset1):\n",
    "        projection1 = project_theta(angles + offset1, ms).sum(1)\n",
    "        evaluate_image_theta1 = evaluate_functions_on_theta(data_theta, projection1, ms)\n",
    "        evaluate_image_theta1 = apply_threshold_to_sin(evaluate_image_theta1, threshold=threshold) # thin out laser if needed\n",
    "        loss1 = -(intensity * evaluate_image_theta1).sum()\n",
    "        opt1.zero_grad()\n",
    "        loss1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "    # Ensure offset1 stays within bounds if needed\n",
    "    adjustment = 60 / 180 * np.pi\n",
    "    if fix_snapback:\n",
    "        with torch.no_grad():\n",
    "            while not (offset1_range[0] <= offset1 <= offset1_range[1]):\n",
    "                offset1 += adjustment if offset1 < offset1_range[0] else -adjustment\n",
    "    ########## end of the first filter\n",
    "\n",
    "    ########## start of the second filter\n",
    "    offset2 = torch.tensor(0., requires_grad=True)\n",
    "    max_iter_offset2 = 101\n",
    "    lr2 = 1e-2\n",
    "    opt2 = torch.optim.Adam([offset2], lr=lr2)\n",
    "    masked_intensity = create_mask_from_intensity(intensity, evaluate_image_theta1) # mask out intensity from the first laser\n",
    "    for i in range(max_iter_offset2):\n",
    "        # Second filter on masked intensity\n",
    "        projection2 = project_theta(angles + offset2, ms).sum(1)\n",
    "        evaluate_image_theta2 = evaluate_functions_on_theta(data_theta, projection2, ms)\n",
    "        evaluate_image_theta2 = apply_threshold_to_sin(evaluate_image_theta2, threshold=threshold) # thin out laser if needed   \n",
    "        loss2 = -(masked_intensity * evaluate_image_theta2).sum()\n",
    "        opt2.zero_grad()\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "    # If no strong signal from masked intensity data then we consider there're no splitting peaks and take offset2 = offset1\n",
    "    # we can discuss more about how to set the condition, I'm not sure if this is the best way\n",
    "    if masked_intensity.max() <= 0.6 * intensity.max():\n",
    "        offset2 = offset1\n",
    "        evaluate_image_theta2 = evaluate_image_theta1\n",
    "    ########## end of the second filter\n",
    "\n",
    "    # Plot with corrected position\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    ax[0].imshow(((evaluate_image_theta1 / evaluate_image_theta1.abs().max() + intensity / intensity.abs().max()).detach()).T, origin='lower')\n",
    "    ax[1].imshow(((evaluate_image_theta2 / evaluate_image_theta2.abs().max() + intensity / intensity.abs().max()).detach()).T, origin='lower')\n",
    "    plt.show()\n",
    "    # print(loss1, loss2)\n",
    "\n",
    "    return offset1, offset2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "# Assume you have got the .npz files\n",
    "ms = torch.arange(12)\n",
    "angles = torch.arange(0, 6) * 2 * torch.pi / 6.\n",
    "\n",
    "# Import images from .npz files\n",
    "# Extract data_theta, doesn't matter what images is extracted since we're just getting theta\n",
    "# data = np.load(r\"C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Angle reference file (random file from Caden)\\image_111010.npz\")['data']\n",
    "data = np.load('images/image_111019.npz')['data']\n",
    "data_theta = torch.atan2(torch.tensor(data[1]), torch.tensor(data[0]))\n",
    "\n",
    "# Extract data file paths\n",
    "# file_path = r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\HDF to npz files\\\\'\n",
    "file_path = \"/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Field_Sweep_data/\"\n",
    "movies = ['Field_29mT.npz', 'Field_31mT.npz', 'Field_32mT.npz', 'Field_33mT.npz', 'Field_37mT.npz']\n",
    "\n",
    "# Define the movie you want to run GD and GS on as gif\n",
    "gif = movies[4]\n",
    "print(gif)\n",
    "movie = np.load(file_path + gif)\n",
    "intensity_data = torch.tensor(movie['data'])\n",
    "print(intensity_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "offset_list1, offset_list2 = [], []\n",
    "offset1 = torch.tensor(0.)\n",
    "offset1.requires_grad = True\n",
    "initial = True\n",
    "\n",
    "# Loop through the movie\n",
    "for index, image in enumerate(intensity_data):\n",
    "    offset1, offset2 = optimize_offset_2filters(image, offset1, threshold=0.8, initial=initial)\n",
    "    print(f'{(index + 1) * 10}s: offset 1 = {np.rad2deg(offset1.item())}, offset 2 = {np.rad2deg(offset2.item())}')\n",
    "    initial = False\n",
    "    offset_list1.append(np.rad2deg(offset1.item())), offset_list2.append(np.rad2deg(offset2.item()))\n",
    "\n",
    "# Plot offset angles\n",
    "time = np.array(range(len(offset_list1))) * 10 + 10\n",
    "print(offset_list1)\n",
    "plt.plot(offset_list1, label=\"offset1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(offset_list1, label=\"offset1\")\n",
    "plt.plot(offset_list2, label=\"offset2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save model data\n",
    "offset_list1=np.array(offset_list1)\n",
    "# file_path = r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\\\'\n",
    "file_path = rf'/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Field_Sweep_data/angles/'\n",
    "full_path = file_path + gif\n",
    "np.savez(full_path, gif, offset=offset_list1, time=time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angular velocity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes are directly from above\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "movies = ['Field_29mT.npz', 'Field_31mT.npz', 'Field_31mT_fix_snapback.npz', 'Field_32mT.npz', 'Field_33mT.npz', 'Field_37mT.npz']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for gif in movies:\n",
    "    # ratchet_model_data = np.load(rf'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\{gif}')\n",
    "    ratchet_model_data = np.load(rf'/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Field_Sweep_data/angles/{gif}')\n",
    "    rm_time = ratchet_model_data['time']\n",
    "    rm_offset = ratchet_model_data['offset']\n",
    "    \n",
    "    m, b = np.polyfit(rm_time, rm_offset, 1)\n",
    "    best_fit = m * rm_time + b\n",
    "    SS_tot = np.sum((rm_offset - np.mean(rm_offset))**2)\n",
    "    SS_res = np.sum((rm_offset - best_fit)**2)\n",
    "    R_squared = 1 - (SS_res / SS_tot)\n",
    "    range = np.abs(np.min(rm_offset)-np.max(rm_offset))\n",
    "    if R_squared >= .95 or range<=8:\n",
    "        ax[0].plot(rm_time, best_fit, label=gif + \" (best fit)\", alpha=.7)\n",
    "    else:\n",
    "        ax[0].plot(rm_time, rm_offset, label=gif, alpha=.7)\n",
    "    ax[1].plot(rm_time, rm_offset, label=gif, alpha=.7)\n",
    "\n",
    "# Formatting the first subplot (best fit)\n",
    "ax[0].set_xlabel('time (s)')\n",
    "ax[0].set_xlim(0, 380)\n",
    "ax[0].set_ylabel('offset angle (deg)')\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(loc='lower left', fontsize=9)\n",
    "ax[0].set_title('Best Fit')\n",
    "\n",
    "# Formatting the second subplot (original data)\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_xlim(0, 380)\n",
    "ax[1].set_ylabel('offset angle (deg)')\n",
    "ax[1].grid(True)\n",
    "ax[1].legend(loc='lower left', fontsize=9)\n",
    "ax[1].set_title('Original Data')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig(r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Exported Figures\\FieldSweepPositions.png')\n",
    "plt.savefig('angles.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def compute_smoothed_derivative(time, offset, window_length=11, polyorder=2):\n",
    "    '''compute velocity of data after savgol_filter is applied'''\n",
    "    smoothed_angle = savgol_filter(offset, window_length=window_length, polyorder=polyorder)\n",
    "    time = np.array(time)\n",
    "    smoothed_derivative = (np.gradient(smoothed_angle, time))\n",
    "    return smoothed_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate angular velo and plot\n",
    "for gif in movies:\n",
    "    # ratchet_model_data = np.load(rf'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\{gif}')\n",
    "    ratchet_model_data = np.load(rf'/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Field_Sweep_data/angles/{gif}')\n",
    "    rm_time = ratchet_model_data['time']\n",
    "    rm_offset = ratchet_model_data['offset']\n",
    "    m, b = np.polyfit(rm_time, rm_offset, 1)\n",
    "    best_fit = m * rm_time + b\n",
    "    SS_tot = np.sum((rm_offset - np.mean(rm_offset))**2)\n",
    "    SS_res = np.sum((rm_offset - best_fit)**2)\n",
    "    R_squared = 1 - (SS_res / SS_tot)\n",
    "    range = np.abs(np.min(rm_offset)-np.max(rm_offset))\n",
    "    #print(f'{gif.removesuffix('.npz')} R^2=', R_squared)\n",
    "    print(f'{gif} R^2=', R_squared)\n",
    "    # print(f'{gif} range=', range)\n",
    "    if R_squared >= .98 or range<=8:\n",
    "        velo = compute_smoothed_derivative(rm_time, best_fit)\n",
    "        #print(f'{gif.removesuffix('.npz')} avg angular velo=', m, 'deg/s')\n",
    "        print(f'{gif} avg angular velo=', m, 'deg/s')\n",
    "    else:\n",
    "        velo = compute_smoothed_derivative(rm_time, rm_offset)\n",
    "\n",
    "    #plt.plot(rm_time, velo, label=gif.removesuffix('.npz'), alpha=.7)\n",
    "    plt.plot(rm_time, velo, label=gif, alpha=.7)\n",
    "\n",
    "plt.xlabel('time (s)')\n",
    "plt.xlim(0,380)\n",
    "plt.ylabel('Angular Velocity (deg/s)')\n",
    "# plt.title('Ratchet Model')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower left')\n",
    "# plt.savefig(r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Exported Figures\\FieldSweepVelocities.png')\n",
    "plt.savefig('angular_velocity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Temp Sweep Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this model works on previous movies\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "def mask_and_blur_images(array):\n",
    "    '''masks signal inside radius of 14 and outside radius of 30 and adds gaussian blur for all intensity data'''\n",
    "    for i in range(0,60):\n",
    "        x,y = np.meshgrid(np.arange(128), np.arange(128))\n",
    "        radius = np.sqrt((x-64)**2 + (y-62)**2)\n",
    "        mask1 = radius <= 14\n",
    "        mask2 = radius >= 30\n",
    "        masked_data = array[i].copy()\n",
    "        masked_data[mask1] = 0\n",
    "        masked_data2 = masked_data.copy()\n",
    "        masked_data2[mask2] = 0\n",
    "        # masked_data_norm = (masked_data - np.min(masked_data) / (np.max(masked_data) - np.min(masked_data)))\n",
    "        blurred_data = gaussian_filter(masked_data2, sigma=.65)\n",
    "        array[i] = blurred_data\n",
    "    return array\n",
    "\n",
    "# Define parameters\n",
    "# Assume you have got the .npz files\n",
    "ms = torch.arange(12)\n",
    "angles = torch.arange(0, 6) * 2 * torch.pi / 6.\n",
    "\n",
    "# Import images from .npz files\n",
    "# Extract data_theta, doesn't matter what images is extracted since we're just getting theta\n",
    "# data = np.load(r\"C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Angle reference file (random file from Caden)\\image_111010.npz\")['data']\n",
    "data = np.load('images/image_111019.npz')['data']\n",
    "data_theta = torch.atan2(torch.tensor(data[1]), torch.tensor(data[0]))\n",
    "\n",
    "# Extract data file paths\n",
    "# file_path = r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\HDF to npz files\\\\'\n",
    "file_path = \"/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Temp_Sweep_data/\"\n",
    "movies = ['118923.npz', '119486.npz', '119996.npz', '120506.npz', '121016.npz', '121405.npz', '121855.npz', '122365.npz', '122875.npz']\n",
    "\n",
    "# Define the movie you want to run GD and GS on as gif\n",
    "gif = movies[2]\n",
    "print(gif)\n",
    "movie = np.load(file_path + gif)\n",
    "intensity_data = torch.tensor(mask_and_blur_images(movie['intensity']))\n",
    "print(intensity_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "offset_list1, offset_list2 = [], []\n",
    "offset1 = torch.tensor(0.)\n",
    "offset1.requires_grad = True\n",
    "initial = True\n",
    "\n",
    "# Loop through the movie\n",
    "# 119486.npz: angle_below_offset=57, angle_above_offset=3\n",
    "# 121016.npz: 50, 10\n",
    "# 121405.npz: angle_below_offset=10, angle_above_offset=50\n",
    "# 120506.npz: angle_below_offset=10, angle_above_offset=50\n",
    "# 121855.npz: angle_below_offset=55, angle_above_offset=5\n",
    "\n",
    "for index, image in enumerate(intensity_data):\n",
    "    offset1, offset2 = optimize_offset_2filters(image, offset1, threshold=-1, \n",
    "                                                initial=initial, fix_snapback=True, angle_below_offset=55, angle_above_offset=5)\n",
    "    print(f'{(index + 1) * 10}s: offset 1 = {np.rad2deg(offset1.item())}, offset 2 = {np.rad2deg(offset2.item())}')\n",
    "    initial = False\n",
    "    offset_list1.append(np.rad2deg(offset1.item())), offset_list2.append(np.rad2deg(offset2.item()))\n",
    "\n",
    "# Plot offset angles\n",
    "time = np.array(range(len(offset_list1))) * 10 + 10\n",
    "print(offset_list1)\n",
    "plt.plot(offset_list1, label=\"offset1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(offset_list1, label=\"offset1\")\n",
    "plt.plot(offset_list2, label=\"offset2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save model data\n",
    "offset_list1=np.array(offset_list1)\n",
    "# file_path = r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\\\'\n",
    "file_path = rf'/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Temp_Sweep_data/angles/'\n",
    "full_path = file_path + gif\n",
    "np.savez(full_path, gif, offset=offset_list1, time=time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
