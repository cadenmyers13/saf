{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This will run if you have the the movies saved as \"start_numor\".npz files. Check other notebook titled SANS_to_npz.ipynb for code to assist in doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to run the notebook, first replace all file paths (check for `np.load` and `file_path`).\n",
    "search for `gif = movies[i]` where this line allows you to change the set of data\n",
    "Parameters to tune: `FILTER_SIGNAL_THRESHOLD`, `laser_threshold`, `angle_above_offset` (in fix snapback section where you can decide if you want to change offsets)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (2 filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from matplotlib.animation import FuncAnimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your file path\n",
    "file_path = '/Users/cadenmyers/billingelab/dev/sym_adapted_filts/experimental_data/46349.npz'\n",
    "data = np.load(file_path)['data']\n",
    "\n",
    "im_shape = data[0].shape\n",
    "# im_shape = (128, 128)\n",
    "x, y = np.meshgrid(\n",
    "    np.arange(-im_shape[0] // 2, im_shape[0] // 2),\n",
    "    np.arange(-im_shape[1] // 2, im_shape[1] // 2),\n",
    ")\n",
    "DATA_THETA = torch.atan2(torch.tensor(x), torch.tensor(y))\n",
    "MAX_ITER_OFFSET = 51\n",
    "LR = 1e-2\n",
    "\n",
    "n_folds = 6\n",
    "k=8.4\n",
    "print(\"n_folds =\", n_folds)\n",
    "print('k value =', k)\n",
    "def filter_function(k, theta, n_folds=n_folds):\n",
    "    filter = torch.exp(k * torch.log((torch.cos(n_folds / 2 * theta))**2))\n",
    "    # plt.imshow(filter)\n",
    "    # plt.title(f'n_folds={n_folds}, k={k}')\n",
    "    # plt.show()\n",
    "    return filter\n",
    "\n",
    "def normalize_min_max(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        array = data.numpy()\n",
    "    else:\n",
    "        array = data\n",
    "\n",
    "    array_min = np.min(array)\n",
    "    array_max = np.max(array)\n",
    "    norm_array = (array - array_min) / (array_max - array_min)\n",
    "\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        norm_tensor = torch.tensor(norm_array)\n",
    "        return norm_tensor\n",
    "    else:\n",
    "        return norm_array\n",
    "\n",
    "def mask_and_blur_images(array, sigma=1):\n",
    "    '''Masks signal inside radius of 14 and outside radius of 30 and adds gaussian blur for all intensity data'''\n",
    "    x, y = np.meshgrid(np.arange(128), np.arange(128))\n",
    "    radius = np.sqrt((x - 64) ** 2 + (y - 62) ** 2)\n",
    "    mask1 = radius <= 14\n",
    "    mask2 = radius >= 30\n",
    "    masked_data = array.copy()\n",
    "    masked_data[mask1] = 0\n",
    "    masked_data2 = masked_data.copy()\n",
    "    masked_data2[mask2] = 0\n",
    "    blurred_data = gaussian_filter(masked_data2, sigma=sigma)\n",
    "    return blurred_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE GD FUNCTION\n",
    "\n",
    "def gradient_descent_optimize_offset(intensity, offset, k=k):\n",
    "    opt = torch.optim.Adam([offset], lr=LR)\n",
    "    for i in range(MAX_ITER_OFFSET):\n",
    "        evaluate_image_theta = filter_function(k, DATA_THETA+offset)\n",
    "        loss = -(intensity * evaluate_image_theta).sum()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return offset, evaluate_image_theta, loss\n",
    "\n",
    "# offset, image, loss = gradient_descent_optimize_offset(intensity_data[0], offset1)\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# im1 = axes[0].imshow(normalize_min_max(intensity_data[0]) - mask_images(image.detach().numpy()), cmap='viridis')\n",
    "# axes[0].axis('off')\n",
    "# fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# im2 = axes[1].imshow(image.detach().numpy(), cmap='viridis')\n",
    "# axes[1].axis('off')\n",
    "# fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# im3 = axes[2].imshow(normalize_min_max(intensity_data[0]), cmap='viridis')\n",
    "# axes[2].axis('off')\n",
    "# fig.colorbar(im3, ax=axes[2])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to fix snapback\n",
    "def adjust_offset_within_bounds(offset_list, angle_above_offset=50):\n",
    "    angle_below_offset = 60 - angle_above_offset\n",
    "    adjusted_offsets = []\n",
    "    prev_offset = offset_list[0]\n",
    "    for index, offset in enumerate(offset_list):\n",
    "        if index == 0:\n",
    "            adjusted_offsets.append(offset)\n",
    "            prev_offset = offset\n",
    "        else:\n",
    "            offset_range = (prev_offset - angle_below_offset, prev_offset + angle_above_offset)\n",
    "            while not (offset_range[0] <= offset <= offset_range[1]):\n",
    "                offset += OFFSET_ADJUSTMENT if offset < offset_range[0] else -OFFSET_ADJUSTMENT\n",
    "            adjusted_offsets.append(offset)\n",
    "            prev_offset = offset\n",
    "    return adjusted_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters for model (usually we don't need to change this)\n",
    "MAX_ITER_OFFSET = 101\n",
    "LR = 1e-2\n",
    "OFFSET_ADJUSTMENT = 60\n",
    "FILTER_SIGNAL_THRESHOLD = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING TWO FILTER GD\n",
    "'''Currently, this code has a penalty function with shape the same as the filter function.\n",
    "Also, a condition is in place such that if the second loss is less than the 30% of the first loss,\n",
    "offset2 = offset1.'''\n",
    "\n",
    "offset1 = torch.tensor(0., requires_grad=True)\n",
    "offset_second = torch.tensor(0., requires_grad=True)\n",
    "penalty_strength = 1.0E6\n",
    "\n",
    "def gradient_descent_optimize_offset(intensity, offset, offset_second, k=k, penalty_strength=penalty_strength):\n",
    "    # First Gradient Descent to find the global maximum\n",
    "    opt = torch.optim.Adam([offset], lr=LR)\n",
    "    for i in range(MAX_ITER_OFFSET):\n",
    "        evaluate_image_theta = filter_function(k, DATA_THETA + offset)\n",
    "        loss = -(intensity * evaluate_image_theta).sum()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # Store the first optimal offset\n",
    "    first_offset = offset.clone().detach()\n",
    "\n",
    "    # Second Gradient Descent with penalty to avoid first peak\n",
    "    opt_second = torch.optim.Adam([offset_second], lr=LR)\n",
    "    for i in range(MAX_ITER_OFFSET):\n",
    "        evaluate_image_theta_second = filter_function(k, DATA_THETA + offset_second)\n",
    "        penalty = penalty_strength * filter_function(1/2*k, offset_second - first_offset)\n",
    "\n",
    "        # Modified loss with penalty\n",
    "        loss_second = -(intensity * evaluate_image_theta_second).sum() + penalty\n",
    "        opt_second.zero_grad()\n",
    "        loss_second.backward()\n",
    "        opt_second.step()\n",
    "\n",
    "    # If the second loss is significantly smaller than the first, set offset_second to first_offset\n",
    "    if np.abs(loss_second.item()) < 0.3 * np.abs(loss.item()):\n",
    "        offset_second = first_offset.clone().detach()\n",
    "        evaluate_image_theta_second = filter_function(k, DATA_THETA + offset_second)\n",
    "\n",
    "    return first_offset, evaluate_image_theta, loss, offset_second, evaluate_image_theta_second, loss_second\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply functions to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import images from .npz files\n",
    "# Extract data file paths\n",
    "file_path = \"/Users/cadenmyers/billingelab/dev/skyrmion_lattices/experimental_data/npz_sept_data/npz_field_sweep/\"\n",
    "\n",
    "# TEMP SWEEP MOVIES\n",
    "# movies = ['121855.npz', '118923.npz', '119486.npz', '119996.npz', '120506.npz', '121016.npz', '121405.npz', '121550.npz', '122365.npz', '122875.npz']\n",
    "\n",
    "# FIELD SWEEP MOVIES OLD\n",
    "#movies = ['Field_29mT.npz', 'Field_31mT.npz', 'Field_32mT.npz', 'Field_33mT.npz', 'Field_37mT.npz']\n",
    "\n",
    "#SEPT DATA\n",
    "movies = ['neg23mT_553_50mW.npz', 'neg23mT_558_25mW.npz', 'neg25mT_553_50mW.npz', 'neg25mT_558_25mW.npz', 'neg27mT_553_50mW.npz', \n",
    "          'neg27mT_558_25mW.npz', 'neg29mT_553_50mW.npz', 'neg29mT_558_25mW.npz', 'neg31mT_553_50mW.npz', 'neg31mT_558_25mW.npz', \n",
    "          'neg33mT_553_50mW.npz', 'neg33mT_558_25mW.npz', 'neg35mT_553_50mW.npz', 'pos23mT_553_50mW.npz', 'pos23mT_558_25mW.npz', \n",
    "          'pos25mT_553_50mW.npz', 'pos25mT_558_25mW.npz', 'pos27mT_553_50mW.npz', 'pos27mT_558_25mW.npz', 'pos29mT_553_50mW.npz', \n",
    "          'pos29mT_558_25mW.npz', 'pos31mT_553_50mW.npz', 'pos31mT_558_25mW.npz', 'pos33mT_553_50mW.npz', 'pos33mT_558_25mW.npz']\n",
    "\n",
    "# Define the movie you want to run GD and GS on as gif (gif = movies[i])\n",
    "#movies = ['pos29mT_558_50mW.npz']\n",
    "gif = movies[15]\n",
    "print(gif)\n",
    "\n",
    "movie = np.load(file_path + gif)\n",
    "intensity_data = torch.tensor(movie['data'])\n",
    "\n",
    "# Parameters:\n",
    "#   iterations: Number of iterations to run the diffusion process.\n",
    "#   kappa: Threshold for edge stopping (higher means less edge detection).\n",
    "#   gamma: Step size (controls diffusion speed).\n",
    "niter=50\n",
    "kappa=30\n",
    "gamma=.1\n",
    "intensity_data = torch.tensor(gaussian_filter(intensity_data, sigma=.4))\n",
    "print('Tensor shape should be (X,128,128), where X is the number of images.')\n",
    "print(intensity_data.shape)\n",
    "plt.imshow(intensity_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD TWO FILTER\n",
    "offset_list1 = []\n",
    "offset_list2 = []\n",
    "first_image_list = []\n",
    "second_image_list = []\n",
    "offset1 = torch.tensor(0.0, requires_grad=True)\n",
    "offset2 = torch.tensor(0.0, requires_grad=True)  # Ensure this is defined\n",
    "previous_offset1 = torch.tensor(0.0)  # Track the previous first_offset\n",
    "k = k_val(resolution)\n",
    "end_frame = 299\n",
    "\n",
    "for index, image in enumerate(intensity_data[:end_frame]):\n",
    "    # Perform gradient descent optimization\n",
    "    offset1, image1, loss1, offset2, image2, loss2 = gradient_descent_optimize_offset(image, offset1, offset2, k=k, penalty_strength=penalty_strength)\n",
    "    \n",
    "    offset1 = torch.tensor(offset1, requires_grad=True)\n",
    "    offset2 = torch.tensor(offset2, requires_grad=True)\n",
    "    \n",
    "    # Check the condition for switching offsets\n",
    "    # if index > 0 and abs(np.rad2deg(offset1.item()) - np.rad2deg(previous_offset1.item())) >= 10:\n",
    "    #     offset1, offset2 = offset2, offset1\n",
    "    #     image1, image2 = image2, image1\n",
    "\n",
    "    # Append results to the lists\n",
    "    offset_list1.append(np.rad2deg(offset1.item()))\n",
    "    offset_list2.append(np.rad2deg(offset2.item()))\n",
    "    first_image_list.append(image1)\n",
    "    second_image_list.append(image2)\n",
    "\n",
    "    # Update previous_offset for the next iteration\n",
    "    previous_offset1 = offset1.clone()  # Store the current first_offset for the next iteration\n",
    "\n",
    "    # Print the current offsets\n",
    "    print(f'{(index + 1) * 10}s: offset 1 = {np.rad2deg(offset1.item())}, offset 2 = {np.rad2deg(offset2.item())}')\n",
    "    print('GD 1 loss:', loss1.item())\n",
    "    print('GD 2 loss:', loss2.item())\n",
    "\n",
    "    # PLOTTING\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    axes[0].imshow(image1.detach().T + normalize_min_max(intensity_data[index]).T, origin='lower')\n",
    "    axes[0].set_title('First GD')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(image2.detach().T + normalize_min_max(intensity_data[index]).T, origin='lower')\n",
    "    axes[1].set_title('Second GD')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(image1.detach().T + image2.detach().T * 2 + normalize_min_max(intensity_data[index]).T, origin='lower')\n",
    "    axes[2].set_title('First + Second GD')\n",
    "    axes[2].axis('off')\n",
    "    axes[3].imshow(intensity_data[index].T, origin='lower')\n",
    "    axes[3].set_title('Intensity Data')\n",
    "    axes[3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(offset_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE GIF\n",
    "\n",
    "frame_ranges = np.arange(0,end_frame,1)\n",
    "# Assuming normalize_min_max, intensity_data, and a way to generate first_image and second_image dynamically are defined\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "# Function to get first_image and second_image based on the current frame index `i`\n",
    "def get_images(i):\n",
    "    first_image = first_image_list[i]  # Replace with actual logic\n",
    "    second_image = second_image_list[i]  # Replace with actual logic\n",
    "    return first_image, second_image\n",
    "# Update function for the animation\n",
    "def update(i):\n",
    "    first_image, second_image = get_images(i)  # Update images each frame\n",
    "    for ax in axes:\n",
    "        ax.clear()\n",
    "    axes[0].imshow(first_image.detach().T + normalize_min_max(intensity_data[i]).T, origin='lower')\n",
    "    axes[0].set_title('First GD')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(second_image.detach().T + normalize_min_max(intensity_data[i]).T, origin='lower')\n",
    "    axes[1].set_title('Second GD')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(first_image.detach().T + second_image.detach().T * 2 + normalize_min_max(intensity_data[i]).T, origin='lower')\n",
    "    axes[2].set_title('First + Second GD')\n",
    "    axes[2].axis('off')\n",
    "    axes[3].imshow(intensity_data[i].T, origin='lower')\n",
    "    axes[3].set_title(f'{gif}, {(i+1)*10}s')\n",
    "    axes[3].axis('off')\n",
    "# Number of frames (change as needed)\n",
    "frames = len(frame_ranges)\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=frames, interval=200)  # Adjust interval for speed (in ms)\n",
    "# Save as GIF, uncomment to save\n",
    "ani.save('hexagon_rotation.gif', writer='pillow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD WITH SUBTRACTING INTENSITIES\n",
    "\n",
    "# Loop through the movie\n",
    "# offset_list1, offset_list2 = [], []\n",
    "# offset1 = torch.tensor(0., requires_grad=True)\n",
    "# offset2 = torch.tensor(0., requires_grad=True)\n",
    "# for index, image in enumerate(intensity_data):\n",
    "#     offset1, offset2, loss1, loss2 = optimize_offset_2filters(image, offset1, offset2)\n",
    "#     print(f'{(index + 1) * 10}s: offset 1 = {np.rad2deg(offset1.item())}, offset 2 = {np.rad2deg(offset2.item())}')\n",
    "#     print('GD 1 loss:', loss1.item())\n",
    "#     print('GD 2 loss:', loss2.item())\n",
    "#     offset_list1.append(np.rad2deg(offset1.item())), offset_list2.append(np.rad2deg(offset2.item()))\n",
    "\n",
    "# # Plot offset angles\n",
    "# time = np.array(range(len(offset_list1))) * 10 + 10\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "# ax[0].plot(offset_list1, label=\"offset1\")\n",
    "# ax[1].plot(offset_list2, label=\"offset2\")\n",
    "# ax[0].legend()\n",
    "# ax[1].legend()\n",
    "# plt.show()\n",
    "\n",
    "# print(offset_list1)\n",
    "# print(offset_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ranges = np.arange(0,end_frame,1)\n",
    "\n",
    "plt.plot(10*(frame_ranges+1), np.array(offset_list1), label = 'offset 1')\n",
    "# offset_list2_fix = adjust_offset_within_bounds(offset_list2, 5) \n",
    "plt.plot(10*(frame_ranges+1), offset_list2, label = 'offset 2')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Offset angle')\n",
    "plt.title(f'{gif}, GD with penalty')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to fix snapback\n",
    "adjusted_offset_list1 = offset_list1#adjust_offset_within_bounds(offset_list1, angle_above_offset=10)\n",
    "adjusted_offset_list2 = offset_list2#adjust_offset_within_bounds(offset_list2, angle_above_offset=20)\n",
    "\n",
    "# Plot offset angles\n",
    "time = np.array(range(len(offset_list1))) * 10 + 10\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "ax[0].plot(adjusted_offset_list1, label=\"adjusted offset1\")\n",
    "ax[1].plot(adjusted_offset_list2, label=\"adjusted offset2\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()\n",
    "\n",
    "# Save model data\n",
    "adjusted_offset_list1=np.array(adjusted_offset_list1)\n",
    "adjusted_offset_list2=np.array(adjusted_offset_list2)\n",
    "# file_path = r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\\\'\n",
    "file_path = rf'/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Field_Sweep_data/angles/'\n",
    "full_path = file_path + gif\n",
    "np.savez(full_path, gif, offset1=adjusted_offset_list1, offset2=adjusted_offset_list2, time=time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angular velocity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Field_29mT.npz', 'Field_31mT.npz', 'Field_32mT.npz', 'Field_33mT.npz','Field_37mT.npz']\n",
    "\n",
    "for gif in movies:\n",
    "    # ratchet_model_data = np.load(rf'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\{gif}')\n",
    "    ratchet_model_data = np.load(rf'/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Field_Sweep_data/angles/{gif}')\n",
    "    rm_time = ratchet_model_data['time']\n",
    "    rm_offset1 = ratchet_model_data['offset1']\n",
    "    rm_offset2 = ratchet_model_data['offset2']\n",
    "    plt.plot(rm_time, rm_offset1, label=f'{gif}, offset1', alpha=.7)\n",
    "    plt.plot(rm_time, rm_offset2, label=f'{gif}, offset2', alpha=.7)\n",
    "\n",
    "plt.xlabel('time (s)')\n",
    "# plt.xlim(0, 380)\n",
    "plt.ylabel('offset angle (deg)')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.5, 0.8))\n",
    "plt.title('Field Sweep offset angle')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Exported Figures\\FieldSweepPositions.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "def compute_smoothed_derivative(time, offset, window_length=11, polyorder=2):\n",
    "    '''compute velocity of data after savgol_filter is applied'''\n",
    "    smoothed_angle = savgol_filter(offset, window_length=window_length, polyorder=polyorder)\n",
    "    time = np.array(time)\n",
    "    smoothed_derivative = (np.gradient(smoothed_angle, time))\n",
    "    return smoothed_derivative\n",
    "\n",
    "# calculate angular velo and plot\n",
    "for gif in movies:\n",
    "    # ratchet_model_data = np.load(rf'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Peak Tracking npz files\\{gif}')\n",
    "    ratchet_model_data = np.load(rf'/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/Field_Sweep_data/angles/{gif}')\n",
    "    rm_time = ratchet_model_data['time']\n",
    "    rm_offset1 = ratchet_model_data['offset1']\n",
    "    rm_offset2 = ratchet_model_data['offset2']\n",
    "    velo1 = compute_smoothed_derivative(rm_time, rm_offset1)\n",
    "    velo2 = compute_smoothed_derivative(rm_time, rm_offset2)\n",
    "    plt.plot(rm_time, velo1, label=f'{gif}, velocity1, average = {np.mean(velo1): .04f}', alpha=.7)\n",
    "    plt.plot(rm_time, velo2, label=f'{gif}, velocity2, average = {np.mean(velo2): .04f}', alpha=.7)\n",
    "\n",
    "plt.xlabel('time (s)')\n",
    "# plt.xlim(0,380)\n",
    "plt.ylabel('Angular Velocity (deg/s)')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.8))\n",
    "# plt.title('Ratchet Model')\n",
    "plt.grid(True)\n",
    "# plt.savefig(r'C:\\Users\\Nathan\\OneDrive - nd.edu\\Desktop\\SANS Data\\Experiments\\PSI Cu2OSeO3 Corbino July 2023\\Analysis\\Field Sweep\\Exported Figures\\FieldSweepVelocities.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skyrmion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
