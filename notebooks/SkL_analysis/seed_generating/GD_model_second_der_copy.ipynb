{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Before running, you will need to download the file `image_theta.npz` from the dropbox. It is under the director `offset seeds and gifs`. This is where we will upload everything. Once downloaded, replace `DATA` with the correct path to `image_theta.npz`. You should be able to run all. This will run gradient descent out-of-box and will create a gif. Use this to see if your assigned data needs to be clicked through. If it doesn't, great! Save the npz and gifs to the dropbox. If it does, go to `click_to_angle.ipynb` and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.ndimage import gaussian_filter\n",
    "# from medpy.filter.smoothing import anisotropic_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global parameters\n",
    "system_symmetry = 6.\n",
    "k = 8.4\n",
    "MS = torch.arange(2*system_symmetry)\n",
    "ANGLES = torch.arange(0, system_symmetry)* 2 * torch.pi / system_symmetry\n",
    "MAX_ITER_OFFSET = 51\n",
    "LR = 1e-2\n",
    "OFFSET_ADJUSTMENT = int(360/system_symmetry)\n",
    "OFFSET_ADJUSTMENT_rad = np.deg2rad(OFFSET_ADJUSTMENT)\n",
    "DATA = np.load(\"/Users/cadenmyers/billingelab/dev/sym_adapted_filts/experimental_data/image_theta.npz\")[\"data\"]\n",
    "# DATA = np.load('/Users/cadenmyers/billingelab/dev/sym_adapted_filts/experimental_data/npz_temp_sweep/image_theta.npz')['data']\n",
    "DATA_THETA = torch.atan2(torch.tensor(DATA[1]), torch.tensor(DATA[0]))\n",
    "\n",
    "# file_path = '/Users/cadenmyers/billingelab/dev/sym_adapted_filts/experimental_data/npz_sept_data/npz_field_sweep/neg29mT_553_50mW.npz'\n",
    "# data = np.load(file_path)['data']\n",
    "print(f'n_folds = {system_symmetry}')\n",
    "print(f'k = {k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: mask and blur images we got from SANS_to_npz.ipynb\n",
    "def mask_and_blur_images(array):\n",
    "    \"\"\"Applies a circular mask and Gaussian blur to intensity data.\"\"\"\n",
    "    x, y = np.meshgrid(np.arange(128), np.arange(128))\n",
    "    radius = np.sqrt((x - 62)**2 + (y - 62)**2)\n",
    "    mask = (radius > 18) & (radius < 30)\n",
    "    for i in range(len(array)):\n",
    "        masked_data = array[i] * mask\n",
    "        array[i] = gaussian_filter(masked_data, sigma=0.65)\n",
    "    return array\n",
    "\n",
    "n_folds=6\n",
    "def filter_function(k, theta, n_folds=n_folds):\n",
    "    filter = torch.exp(k * torch.log((torch.cos(n_folds / 2 * theta))**2))\n",
    "    # plt.imshow(filter)\n",
    "    # plt.title(f'n_folds={n_folds}, k={k}')\n",
    "    # plt.show()\n",
    "    return filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/Users/yucongchen/billingegroup/skyrmion_lattices/skyrmion-lattices-data/sep_data/\"\n",
    "file_path = '/Users/cadenmyers/billingelab/dev/sym_adapted_filts/experimental_data/'\n",
    "file_path = '/Users/cadenmyers/billingelab/dev/sym_adapted_filts/experimental_data/npz_sept_numor_data/'\n",
    "numor='56059'\n",
    "movies = numor + '.npz'\n",
    "# Define the movie you want to run GD and GS on as gif (gif = movies[i])\n",
    "gif = movies\n",
    "print(gif)\n",
    "movie = np.load(file_path + gif)\n",
    "intensity_data = torch.tensor(mask_and_blur_images(movie[\"data\"]))\n",
    "filename, _ = os.path.splitext(gif)\n",
    "# print(filename)\n",
    "\n",
    "print(\"Tensor shape should be (X,128,128), where X is the number of images.\")\n",
    "print(intensity_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 2: get correct period indices\n",
    "def get_correct_period_indices(loss_values):\n",
    "    \"\"\"Finds the correct period indices based on the maximum value in the loss values.\n",
    "    If only one max index is found, adds adjacent indices at -60 or +60.\"\"\"\n",
    "    indices = [i for i, loss in enumerate(loss_values) if loss == max(loss_values)]\n",
    "    if len(indices) > 1:\n",
    "        return sorted(indices[:2])\n",
    "    index = indices[0]\n",
    "    adjacent_indices = [i for i in [index + OFFSET_ADJUSTMENT, index - OFFSET_ADJUSTMENT] if 0 <= i < len(loss_values)]\n",
    "    return sorted([index] + adjacent_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 3: Discrete search for number of domains and get approximate offset positions\n",
    "def compute_closest_periodic_loss(intensity, prev_offset):\n",
    "    \"\"\"Compute the loss, first derivative, and second derivative for a range of offsets\n",
    "    around the previous offset, selecting the closest periodic range.\"\"\"\n",
    "    \n",
    "    # Define the periodic range and step size for sweeping through offset angles\n",
    "    angle_period = np.deg2rad(360/system_symmetry)\n",
    "    step_size_radians = np.deg2rad(1)\n",
    "    offset_values = np.arange(prev_offset-angle_period, prev_offset+angle_period, step_size_radians)\n",
    "    loss_values = []\n",
    "    for offset in offset_values:\n",
    "        evaluate_image_theta = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset)))**2))\n",
    "        loss = -(intensity * evaluate_image_theta).sum()\n",
    "        loss_values.append(loss.item())\n",
    "    first_derivative = np.gradient(loss_values)\n",
    "    second_derivative = np.gradient(first_derivative)\n",
    "    selected_indices = get_correct_period_indices(loss_values)\n",
    "\n",
    "    # Select the appropriate range of offsets, losses, and derivatives based on the period indices\n",
    "    offset_values = np.array(offset_values[selected_indices[0]:selected_indices[1]+1])\n",
    "    loss_values = np.array(loss_values[selected_indices[0]:selected_indices[1]+1])\n",
    "    first_derivative = np.array(first_derivative[selected_indices[0]:selected_indices[1]+1])\n",
    "    second_derivative = np.array(second_derivative[selected_indices[0]:selected_indices[1]+1])\n",
    "\n",
    "    return offset_values, loss_values, first_derivative, second_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 4: Get approximate offset positions through finding local maxima in second derivatives\n",
    "def get_approx_offset_values(offset_values, second_derivative):\n",
    "    \"\"\"Identify approximate offset positions by finding local maxima and inflection points in the second derivative.\"\"\"\n",
    "    local_maxima_indices = argrelextrema(second_derivative, np.greater)[0]\n",
    "    # Remove indices close to the edges because these local maximas might be introduced by the local maxima from the phase change\n",
    "    filtered_indices = local_maxima_indices[(local_maxima_indices >= 5) & (local_maxima_indices <= len(second_derivative) - 6)]\n",
    "    indices_info = [(index, offset_values[index], second_derivative[index]) for index in filtered_indices]\n",
    "    # Sort by second derivative values (descending) and return top two offsets\n",
    "    sorted_maxima = sorted(indices_info, key=lambda x: x[2], reverse=True)\n",
    "    approx_offset_values = [offset for _, offset, _ in sorted_maxima][:2]\n",
    "    return approx_offset_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 5: Get approximate offset value for the other offset\n",
    "def get_approximate_offset2(current_offset, approx_offset_values, reference_offset):\n",
    "    \"\"\"Calculate an approximate second offset based on provided offsets and a reference offset.\"\"\"\n",
    "    # Get approximate offset value for second offset\n",
    "    offset_range = (reference_offset - OFFSET_ADJUSTMENT_rad/2, reference_offset + OFFSET_ADJUSTMENT_rad/2 + 0.1)\n",
    "    # Edit the values (+- 60 deg.) so that we know which is the first offset\n",
    "    adjusted_offsets = [\n",
    "        offset + (OFFSET_ADJUSTMENT_rad if offset < offset_range[0] else -OFFSET_ADJUSTMENT_rad) * \n",
    "        int(not (offset_range[0] <= offset <= offset_range[1]))\n",
    "        for offset in approx_offset_values\n",
    "    ]\n",
    "    # Calculate the differences to determine the second offset\n",
    "    closest_offset = min(adjusted_offsets, key=lambda x: abs(x - reference_offset))\n",
    "    adjusted_offsets = [offset for offset in adjusted_offsets if offset != closest_offset]\n",
    "    return adjusted_offsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 6: helper function to visualize loss plot and its derivatives\n",
    "def visualize_loss_plot_in_degrees(offset_values, loss_values, first_derivative, second_derivative):\n",
    "    offset_values_deg = np.rad2deg(offset_values)\n",
    "    fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "    axes[0].plot(offset_values_deg, loss_values)\n",
    "    axes[0].set_xlabel(\"Offsets (degrees)\")\n",
    "    axes[0].set_title(\"Loss Values\")\n",
    "    axes[1].plot(offset_values_deg, first_derivative)\n",
    "    axes[1].set_xlabel(\"Offsets (degrees)\")\n",
    "    axes[1].set_title(\"First Derivative\")\n",
    "    axes[2].plot(offset_values_deg, second_derivative)\n",
    "    axes[2].set_xlabel(\"Offsets (degrees)\")\n",
    "    axes[2].set_title(\"Second Derivative\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 7: helper function to normalize data\n",
    "def normalize_min_max(data):\n",
    "    array = data.detach().numpy() if isinstance(data, torch.Tensor) else np.array(data)\n",
    "    norm_array = (array - array.min()) / (array.max() - array.min())\n",
    "    return torch.tensor(norm_array) if isinstance(data, torch.Tensor) else norm_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 8: helper function to fix snapback (assume offsets are in degree)\n",
    "def adjust_offset_within_bounds(offset_list, angle_above_offset=50):\n",
    "    angle_below_offset = OFFSET_ADJUSTMENT - angle_above_offset\n",
    "    adjusted_offsets = []\n",
    "    prev_offset = offset_list[0]\n",
    "    for index, offset in enumerate(offset_list):\n",
    "        if index == 0:\n",
    "            adjusted_offsets.append(offset)\n",
    "            prev_offset = offset\n",
    "        else:\n",
    "            offset_range = (prev_offset - angle_below_offset, prev_offset + angle_above_offset)\n",
    "            while not (offset_range[0] <= offset <= offset_range[1]):\n",
    "                offset += OFFSET_ADJUSTMENT if offset < offset_range[0] else -OFFSET_ADJUSTMENT\n",
    "            adjusted_offsets.append(offset)\n",
    "            prev_offset = offset\n",
    "    return adjusted_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 9: helper function to compute uncertainty\n",
    "def compute_uncertainty(intensity, computed_offset):\n",
    "    \"\"\"Compute uncertainty using curvature: sigma = 1/sqrt(|H|).\"\"\"\n",
    "    angle_period = np.deg2rad(10)\n",
    "    step_size_radians = np.deg2rad(1)\n",
    "    offset_values = np.arange(computed_offset.item()-angle_period, computed_offset.item()+angle_period, step_size_radians)\n",
    "    loss_values = []\n",
    "    for offset in offset_values:\n",
    "        evaluate_image_theta = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset)))**2))\n",
    "        loss = -(intensity * evaluate_image_theta).sum()\n",
    "        loss_values.append(loss.item())\n",
    "    first_derivative = np.gradient(loss_values)\n",
    "    second_derivative = np.gradient(first_derivative)\n",
    "    uncertainty = 1.0 / np.sqrt(np.abs(second_derivative[10])) / np.pi * 180\n",
    "    return uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def penalty_function(offset, penalty_weight=1, tolerance=-5):\n",
    "    '''\n",
    "    if movie has negative offsets, tolerance should be negative,\n",
    "    vice versa for positive offsets.\n",
    "\n",
    "    input tolerance in degrees\n",
    "    input offset is a torch.tensor in radians\n",
    "    '''\n",
    "    offset = np.rad2deg(offset.clone().detach().numpy())\n",
    "    penalty = penalty_weight*np.exp(offset+tolerance)\n",
    "    penalty = torch.tensor(penalty)\n",
    "    return penalty\n",
    "\n",
    "# print('penalty = ', penalty_function(np.pi/24).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 10:\n",
    "\n",
    "def gradient_descent_optimize_offset(intensity, offset1, offset2, k):\n",
    "    \"\"\"\n",
    "    offset1 has the greatest loss score. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Store previous offsets\n",
    "    prev_offset1 = offset1.item()\n",
    "    prev_offset2 = offset2.item()\n",
    "    \n",
    "    # Get approximate offsets\n",
    "    offset_range, loss_values, first_derivative, second_derivative = compute_closest_periodic_loss(intensity, prev_offset1)\n",
    "    approx_offset_values = get_approx_offset_values(offset_range, second_derivative)\n",
    "    number_of_domains = len(approx_offset_values)\n",
    "    # visualize_loss_plot_in_degrees(offset_range, loss_values, first_derivative, second_derivative)\n",
    "    offset_range2, _, _, second_derivative2 = compute_closest_periodic_loss(intensity, prev_offset2)\n",
    "    approx_offset_values2 = get_approx_offset_values(offset_range2, second_derivative2)\n",
    "    # print(\"Number of domains = \", number_of_domains, \", Offset values are approx. \", approx_offset_values, approx_offset_values2)\n",
    "    \n",
    "    # Take care of errorous data\n",
    "    if number_of_domains == 0:\n",
    "        return offset1, offset2, intensity, intensity, 0, 0, 0, 0\n",
    "\n",
    "    # GD1\n",
    "    opt = torch.optim.Adam([offset1], lr=LR)\n",
    "    for i in range(MAX_ITER_OFFSET):\n",
    "        evaluate_image_theta = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset1)))**2)) # sin(3x)^{2k}\n",
    "        intensity = normalize_min_max(intensity.clone().detach())\n",
    "        # Only penalize if offset1 is less than -tolerance\n",
    "        penalty = penalty_function(offset1)\n",
    "        loss = -(intensity * evaluate_image_theta).sum() + penalty\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    if len(approx_offset_values) > 1:\n",
    "        offset1_tmp_value = get_approximate_offset2(prev_offset1, approx_offset_values, offset1.item())\n",
    "        offset1_tmp = torch.tensor(offset1_tmp_value, requires_grad=True)\n",
    "        opt_tmp = torch.optim.Adam([offset1_tmp], lr=LR)\n",
    "        for i in range(10):\n",
    "            evaluate_image_theta_tmp = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset1_tmp)))**2))\n",
    "            loss_tmp = -(intensity * evaluate_image_theta_tmp).sum()\n",
    "            opt_tmp.zero_grad()\n",
    "            loss_tmp.backward()\n",
    "            opt_tmp.step()\n",
    "        if loss_tmp.item() <= loss.item():\n",
    "            # print(offset1_tmp, offset1, \"offset1 switched!\")\n",
    "            loss = loss_tmp\n",
    "            offset1 = offset1_tmp\n",
    "            evaluate_image_theta = evaluate_image_theta_tmp\n",
    "\n",
    "    # GD2\n",
    "    opt2 = torch.optim.Adam([offset2], lr=LR)\n",
    "    for i in range(MAX_ITER_OFFSET):\n",
    "        evaluate_image_theta2 = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset2)))**2))\n",
    "        loss2 = -(intensity * evaluate_image_theta2).sum()\n",
    "        opt2.zero_grad()\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "    if len(approx_offset_values2) > 1:\n",
    "        offset2_tmp_value = get_approximate_offset2(prev_offset2, approx_offset_values2, offset2.item())\n",
    "        offset2_tmp = torch.tensor(offset2_tmp_value, requires_grad=True)\n",
    "        opt2_tmp = torch.optim.Adam([offset2_tmp], lr=LR)\n",
    "        for i in range(10):\n",
    "            evaluate_image_theta2_tmp = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset2_tmp)))**2))\n",
    "            loss2_tmp = -(intensity * evaluate_image_theta2_tmp).sum()\n",
    "            opt2_tmp.zero_grad()\n",
    "            loss2_tmp.backward()\n",
    "            opt2_tmp.step()\n",
    "        if loss2_tmp.item() >= loss2.item():\n",
    "            # print(offset2_tmp, offset2, \"offset2 switched!\")\n",
    "            loss2 = loss2_tmp\n",
    "            offset2 = offset2_tmp\n",
    "            evaluate_image_theta2 = evaluate_image_theta2_tmp\n",
    "\n",
    "    uncertainty1 = compute_uncertainty(intensity, offset1)\n",
    "    uncertainty2 = compute_uncertainty(intensity, offset2)\n",
    "    # print(penalty)\n",
    "    return offset1, offset2, evaluate_image_theta, evaluate_image_theta2, loss.item(), loss2.item(), uncertainty1, uncertainty2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function 10:\n",
    "def flipped_gradient_descent_optimize_offset(intensity, offset1, offset2, k):\n",
    "    \"\"\"\n",
    "    offset2 has the greatest loss score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store previous offsets\n",
    "    prev_offset1 = offset1.item()\n",
    "    prev_offset2 = offset2.item()\n",
    "    \n",
    "    # Get approximate offsets\n",
    "    offset_range, loss_values, first_derivative, second_derivative = compute_closest_periodic_loss(intensity, prev_offset1)\n",
    "    approx_offset_values = get_approx_offset_values(offset_range, second_derivative)\n",
    "    number_of_domains = len(approx_offset_values)\n",
    "    # visualize_loss_plot_in_degrees(offset_range, loss_values, first_derivative, second_derivative)\n",
    "    offset_range2, _, _, second_derivative2 = compute_closest_periodic_loss(intensity, prev_offset2)\n",
    "    approx_offset_values2 = get_approx_offset_values(offset_range2, second_derivative2)\n",
    "    # print(\"Number of domains = \", number_of_domains, \", Offset values are approx. \", approx_offset_values, approx_offset_values2)\n",
    "    \n",
    "    # Take care of errorous data\n",
    "    if number_of_domains == 0:\n",
    "        return offset1, offset2, intensity, intensity, 0, 0, 0, 0\n",
    "\n",
    "    # GD1\n",
    "    opt = torch.optim.Adam([offset1], lr=LR)\n",
    "    for i in range(MAX_ITER_OFFSET):\n",
    "        evaluate_image_theta = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset1)))**2)) # sin(3x)^{2k}\n",
    "        intensity = normalize_min_max(intensity.clone().detach())\n",
    "        # Only penalize if offset1 is less than -tolerance\n",
    "        penalty = penalty_function(offset1)\n",
    "        loss = -(intensity * evaluate_image_theta).sum() + penalty\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    if len(approx_offset_values) > 1:\n",
    "        offset1_tmp_value = get_approximate_offset2(prev_offset1, approx_offset_values, offset1.item())\n",
    "        offset1_tmp = torch.tensor(offset1_tmp_value, requires_grad=True)\n",
    "        opt_tmp = torch.optim.Adam([offset1_tmp], lr=LR)\n",
    "        for i in range(10):\n",
    "            evaluate_image_theta_tmp = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset1_tmp)))**2))\n",
    "            loss_tmp = -(intensity * evaluate_image_theta_tmp).sum()\n",
    "            opt_tmp.zero_grad()\n",
    "            loss_tmp.backward()\n",
    "            opt_tmp.step()\n",
    "        if loss_tmp.item() >= loss.item():\n",
    "            # print(offset1_tmp, offset1, \"offset1 switched!\")\n",
    "            loss = loss_tmp\n",
    "            offset1 = offset1_tmp\n",
    "            evaluate_image_theta = evaluate_image_theta_tmp\n",
    "\n",
    "    # GD2\n",
    "    opt2 = torch.optim.Adam([offset2], lr=LR)\n",
    "    for i in range(MAX_ITER_OFFSET):\n",
    "        evaluate_image_theta2 = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset2)))**2))\n",
    "        loss2 = -(intensity * evaluate_image_theta2).sum()\n",
    "        opt2.zero_grad()\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "    if len(approx_offset_values2) > 1:\n",
    "        offset2_tmp_value = get_approximate_offset2(prev_offset2, approx_offset_values2, offset2.item())\n",
    "        offset2_tmp = torch.tensor(offset2_tmp_value, requires_grad=True)\n",
    "        opt2_tmp = torch.optim.Adam([offset2_tmp], lr=LR)\n",
    "        for i in range(10):\n",
    "            evaluate_image_theta2_tmp = torch.exp(k * torch.log((torch.sin((system_symmetry/2) * (DATA_THETA + offset2_tmp)))**2))\n",
    "            loss2_tmp = -(intensity * evaluate_image_theta2_tmp).sum()\n",
    "            opt2_tmp.zero_grad()\n",
    "            loss2_tmp.backward()\n",
    "            opt2_tmp.step()\n",
    "        if loss2_tmp.item() <= loss2.item():\n",
    "            # print(offset2_tmp, offset2, \"offset2 switched!\")\n",
    "            loss2 = loss2_tmp\n",
    "            offset2 = offset2_tmp\n",
    "            evaluate_image_theta2 = evaluate_image_theta2_tmp\n",
    "\n",
    "    uncertainty1 = compute_uncertainty(intensity, offset1)\n",
    "    uncertainty2 = compute_uncertainty(intensity, offset2)\n",
    "    # print(loss.item(), loss2.item())\n",
    "    return offset1, offset2, evaluate_image_theta, evaluate_image_theta2, loss.item(), loss2.item(), uncertainty1, uncertainty2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset1 = torch.tensor(0., requires_grad=True)\n",
    "offset2 = torch.tensor(0., requires_grad=True)\n",
    "    offset1, offset2, image1, image2, loss1, loss2, u1, u2 = gradient_descent_optimize_offset(intensity_data[0], offset1, offset2, k)\n",
    "    print(np.rad2deg(offset1.item()))\n",
    "    plt.imshow(normalize_min_max(intensity_data[0]) + image1.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply functions\n",
    "# Initialize offsets\n",
    "offset1 = torch.tensor(0., requires_grad=True)\n",
    "offset2 = torch.tensor(0., requires_grad=True)\n",
    "tolerance = torch.tensor(30) # in degrees\n",
    "\n",
    "print(k, system_symmetry, numor, tolerance.item())\n",
    "\n",
    "# Initialize lists\n",
    "frame_ranges = np.arange(0, len(intensity_data), 1)\n",
    "offset_list1, offset_list2 = [], []\n",
    "image_list1, image_list2 = [], []\n",
    "loss_list1, loss_list2 = [], []\n",
    "uncertainty_list1, uncertainty_list2 = [], []\n",
    "\n",
    "for i in frame_ranges:\n",
    "    if i == 0:\n",
    "        offset1, offset2, image1, image2, loss1, loss2, u1, u2 = gradient_descent_optimize_offset(intensity_data[i], offset1, offset2, k)\n",
    "        offset_list1.append(offset1.item())\n",
    "        offset_list2.append(offset2.item())\n",
    "        image_list1.append(image1)\n",
    "        image_list2.append(image2)\n",
    "        loss_list1.append(loss1)\n",
    "        loss_list2.append(loss2)\n",
    "        uncertainty_list1.append(u1)\n",
    "        uncertainty_list2.append(u2)\n",
    "    else:\n",
    "        # Run gradient descent for the current frame\n",
    "        offset1_new, offset2_new, image1_new, image2_new, loss1_new, loss2_new, u1_new, u2_new = gradient_descent_optimize_offset(intensity_data[i], offset1, offset2, k)\n",
    "\n",
    "        # Ensure there is a previous offset to compare against\n",
    "        if len(offset_list1) > 0:\n",
    "            previous_offset1 = torch.tensor(offset_list1[-1])\n",
    "            previous_offset2 = torch.tensor(offset_list2[-1])\n",
    "\n",
    "            # Check if offset1 moved in the opposite direction by more than tolerance\n",
    "            if torch.rad2deg(offset1_new - previous_offset1) > tolerance or torch.rad2deg(offset2_new - previous_offset2) > tolerance or torch.abs(torch.rad2deg(offset1_new - previous_offset1)) > 25:\n",
    "                print('------------')\n",
    "                print(f'{i}: flip')\n",
    "                print(round(torch.rad2deg(offset1_new - previous_offset1).item(), 3), '>', tolerance.item())\n",
    "                    #    'or',round(torch.rad2deg(offset2_new - previous_offset2).item(),3), '>', tolerance.item() )\n",
    "\n",
    "                # If so, rerun with the flipped function\n",
    "                offset1_new, offset2_new, image1_new, image2_new, loss1_new, loss2_new, u1_new, u2_new = flipped_gradient_descent_optimize_offset(intensity_data[i], offset1, offset2, k)\n",
    "\n",
    "        # Append results (store in radians)\n",
    "        # print(offset1_new.item(),offset2_new.item())\n",
    "        offset_list1.append(offset1_new.item())\n",
    "        offset_list2.append(offset2_new.item())\n",
    "        image_list1.append(image1_new)\n",
    "        image_list2.append(image2_new)\n",
    "        loss_list1.append(loss1_new)\n",
    "        loss_list2.append(loss2_new)\n",
    "        uncertainty_list1.append(u1_new)\n",
    "        uncertainty_list2.append(u2_new)\n",
    "\n",
    "        # Update offsets for next iteration\n",
    "        offset1, offset2 = offset1_new, offset2_new\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sava data\n",
    "\n",
    "offset1cm = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/cm_{numor}_offset1_seed.npz')['data']\n",
    "offset2cm = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/cm_{numor}_offset2_seed.npz')['data']\n",
    "offset1yc = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/yc_{numor}_offset2_seed.npz')['data']\n",
    "offset2yc = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/yc_{numor}_offset1_seed.npz')['data']\n",
    "offset1dr = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/dr_{numor}_offset2_seed.npz')['data']\n",
    "offset2dr = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/dr_{numor}_offset1_seed.npz')['data']\n",
    "offset1nc = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/nc_{numor}_offset2_seed.npz')['data']\n",
    "offset2nc = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/working_code/seed_generating/seed_data/nc_{numor}_offset1_seed.npz')['data']\n",
    "offset1oob = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/analysis-generated_data/{numor}_offsets.npz')['offset1']\n",
    "offset2oob = np.load(f'/Users/cadenmyers/billingelab/dev/sym_adapted_filts/analysis-generated_data/{numor}_offsets.npz')['offset2']\n",
    "# (1) Visualize offsets (post-process if necessary)\n",
    "# offset_list1_fix = adjust_offset_within_bounds(offset_list1, 35)\n",
    "# offset_list2_fix = adjust_offset_within_bounds(offset_list2, 30)\n",
    "\n",
    "colors = {\n",
    "    \"cm\": \"blue\",\n",
    "    \"yc\": \"red\",\n",
    "    \"dr\": \"green\",\n",
    "    \"nc\": \"purple\",\n",
    "    \"oob\": 'orange',\n",
    "    \"error\": 'hotpink'\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(offset1oob-offset1oob[0], color=colors[\"oob\"], label=f'offset1 oob {numor}', linestyle='dotted')\n",
    "plt.plot(offset2oob-offset2oob[0], alpha=0.5, color=colors[\"oob\"], label=f'offset2 oob {numor}', linestyle='-')\n",
    "# plt.errorbar(np.arange(0, len(offset_list1)), np.rad2deg(offset_list1) - np.rad2deg(offset_list1)[0], yerr=uncertainty_list1, label=f\"offset1 {numor} GDratchet\", color=colors[\"error\"], linestyle='dotted', capsize=2)\n",
    "# plt.errorbar(np.arange(0, len(offset_list2)), np.rad2deg(offset_list2) - np.rad2deg(offset_list2)[0], yerr=uncertainty_list2, label=f\"offset2 {numor} GDratchet\", color=colors[\"error\"], linestyle='-', capsize=2)\n",
    "# plt.plot(np.rad2deg(offset_list1) - np.rad2deg(offset_list1)[0], label=f\"offset1 {numor} GDratchet\", color=colors[\"error\"], linestyle='dotted')\n",
    "# plt.plot(np.rad2deg(offset_list2) - np.rad2deg(offset_list2)[0], label=f\"offset2 {numor} GDratchet\", color=colors[\"error\"], linestyle='-')\n",
    "\n",
    "plt.plot(offset1cm-offset1cm[0], label=f'offset1 cm {numor}', color=colors['cm'], linestyle='dotted')\n",
    "plt.plot(offset2cm-offset2cm[0], label=f'offset2 cm {numor}', color=colors['cm'], linestyle='-')\n",
    "plt.plot(offset1yc-offset1yc[0], label=f'offset1 yc {numor}', color=colors[\"yc\"], linestyle='dotted')\n",
    "plt.plot(offset2yc-offset2yc[0], label=f'offset2 yc {numor}', color=colors[\"yc\"], linestyle='-')\n",
    "plt.plot(offset1dr-offset1dr[0], label=f'offset1 dr {numor}', color=colors[\"dr\"], linestyle='dotted')\n",
    "plt.plot(offset2dr-offset2dr[0], label=f'offset2 dr {numor}', color=colors[\"dr\"], linestyle='-')\n",
    "plt.plot(offset1nc-offset1nc[0], label=f'offset1 nc {numor}', color=colors[\"nc\"], linestyle='dotted')\n",
    "plt.plot(offset2nc-offset2nc[0], label=f'offset2 nc {numor}', color=colors[\"nc\"], linestyle='-')\n",
    "\n",
    "\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"offset\")\n",
    "plt.title(f\"{numor}, iterations={MAX_ITER_OFFSET}, k={k}, tolerance={tolerance}deg\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # (2) Visualize loss values\n",
    "# plt.plot(np.array(loss_list1), label=\"Loss1\")\n",
    "# plt.plot(np.array(loss_list2), label=\"Loss2\")\n",
    "# plt.xlabel(\"Frames\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# # plt.show()\n",
    "\n",
    "# np.savez(f\"{filename}_offsets.npz\", \n",
    "#          offset1=np.array(offset_list1), offset2=np.array(offset_list2), \n",
    "#          loss1=np.array(loss_list1),loss2=np.array(loss_list2), \n",
    "#          uncertainty1=np.array(uncertainty_list1), uncertainty2=np.array(uncertainty_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.widgets import Slider\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "offsets1 = np.array(offset_list1)\n",
    "# offsets1 = np.rad2deg(offsets1)\n",
    "\n",
    "offsets2 = np.array(offset_list2)\n",
    "# offsets2 = np.rad2deg(offsets2)\n",
    "n_folds = 1\n",
    "k=500\n",
    "print(\"n_folds =\", n_folds)\n",
    "print('k value =', k)\n",
    "\n",
    "\n",
    "\n",
    "def interactive_plot(frame_idx):\n",
    "    '''Plot the intensity data and filter images with respect to the selected frame index.'''\n",
    "    \n",
    "    image1 = filter_function(k, DATA_THETA + offsets1[frame_idx])\n",
    "    image2 = filter_function(k, DATA_THETA + offsets2[frame_idx])\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the first image (combined filter + intensity data)\n",
    "    ax1.imshow(image1 + 2*image2 + normalize_min_max(intensity_data[frame_idx]), origin=\"lower\")\n",
    "    ax1.set_title(f\"Frame {frame_idx+1}: Filter + Intensity\")\n",
    "    ax1.axis(\"off\")\n",
    "    \n",
    "    # Plot the second image (raw intensity data)\n",
    "    ax2.imshow(intensity_data[frame_idx], origin=\"lower\")\n",
    "    ax2.set_title(f\"Frame {frame_idx+1}: Intensity Data\")\n",
    "    ax2.axis(\"off\")\n",
    "    \n",
    "    # Update the figure title with offset values\n",
    "    fig.suptitle(f\"Time={(frame_idx+1)*10}s, Offset1={round(np.rad2deg(offsets1[frame_idx]), 3)}°, Offset2={round(np.rad2deg(offsets2[frame_idx]), 3)}°\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive plot with sliders for frame index\n",
    "interact(\n",
    "    interactive_plot,\n",
    "    frame_idx=IntSlider(value=0, min=0, max=len(intensity_data)-1, step=1, description='Frame Index')\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save gif\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "offsets1 = np.array(offset_list1)\n",
    "offsets1 = np.deg2rad(offsets1)\n",
    "offsets2 = np.array(offset_list2)\n",
    "offsets2 = np.deg2rad(offsets2)\n",
    "\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    image1 = filter_function(k, DATA_THETA+offsets1[i])\n",
    "    image2 = filter_function(k, DATA_THETA+offsets2[i])\n",
    "    for ax in axes:\n",
    "        ax.clear()\n",
    "    axes[0].imshow(image1 + image2 + normalize_min_max(intensity_data[i]), origin=\"lower\")\n",
    "    axes[0].set_title(\"GD\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(intensity_data[i], origin=\"lower\")\n",
    "    axes[1].set_title(\"Intensity Data\")\n",
    "    axes[1].axis(\"off\")\n",
    "    fig.suptitle(f\"Time={(i+1)*10}s, Offset1={round(offsets1[i],3)}deg, Offset2={round(offsets2[i],3)}deg\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# ani = FuncAnimation(fig, update, frames=len(intensity_data), interval=200)\n",
    "# ani.save(f'{filename}_scrap.gif', writer=\"pillow\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skyrmion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
